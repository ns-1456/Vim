{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Local kernel check\n",
        "Open this notebook **from the project root folder** (the folder that contains `data/`, `utils/`, `Vim/`). Then **Run All**. The Drive cell will no-op locally. For a quick sanity run, set `config[\"epochs\"] = 2` in the config cell before running."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vision Mamba (Vim) Training on Colab\n",
        "\n",
        "This notebook trains a Vision Mamba model on CIFAR-100 using Google Colab's free GPU.\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "1. **Upload your project**: Upload the entire `Vim` folder to Colab (or clone from GitHub)\n",
        "2. **Run all cells**: Execute cells sequentially\n",
        "3. **Monitor training**: Check TensorBoard logs or print statements\n",
        "4. **Download checkpoints**: Save model checkpoints to Google Drive or download locally"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch torchvision timm einops tqdm matplotlib tensorboard -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: Mount Google Drive\n",
        "\n",
        "Run this if you want to save checkpoints or use data from Drive. Then set `config[\"output_dir\"]` and/or `config[\"data_dir\"]` to paths under `/content/drive/MyDrive/...`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Only runs on Colab; skip on local kernel\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "except Exception:\n",
        "    print(\"Not in Colab — skipping Drive mount. (Optional: use local paths for data/checkpoints.)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colab: run this first to clone the repo (skip if you already uploaded the project)\n",
        "# Local: skip this cell or it will no-op (no /content).\n",
        "import os\n",
        "if os.path.exists(\"/content\"):  # Colab\n",
        "    if not os.path.exists(\"/content/Vim/data\") or not os.path.exists(\"/content/Vim/Vim\"):\n",
        "        import subprocess\n",
        "        subprocess.run([\"git\", \"clone\", \"https://github.com/ns-1456/Vim.git\", \"/content/Vim\"], check=False)\n",
        "    if os.path.exists(\"/content/Vim\"):\n",
        "        os.chdir(\"/content/Vim\")\n",
        "print(\"Working directory:\", os.getcwd())\n",
        "print(\"Has data/:\", os.path.exists(\"data\"), \"| Has Vim/:\", os.path.exists(\"Vim\") or os.path.exists(\"vim\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Setup Project Structure\n",
        "\n",
        "**Option A**: If you uploaded files, make sure the project structure is correct.\n",
        "\n",
        "**Option B**: If cloning from GitHub, uncomment and modify the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /content\n",
            "Contents: ['.config', 'sample_data']\n"
          ]
        }
      ],
      "source": [
        "# If you uploaded the Vim folder to Colab, it may be at /content/Vim.\n",
        "# If you cloned a repo, %cd into the folder that contains vim/, data/, utils/.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Ensure project root is on path (folder containing Vim/, data/, utils/)\n",
        "def _find_project_root():\n",
        "    if os.path.exists(\"data\") and (os.path.exists(\"vim\") or os.path.exists(\"Vim\")):\n",
        "        return os.getcwd()\n",
        "    for path in [\"/content/Vim\", \"/content\", os.getcwd()]:\n",
        "        data_ok = os.path.exists(os.path.join(path, \"data\"))\n",
        "        vim_ok = os.path.exists(os.path.join(path, \"vim\")) or os.path.exists(os.path.join(path, \"Vim\"))\n",
        "        if data_ok and vim_ok:\n",
        "            return path\n",
        "    return os.getcwd()\n",
        "\n",
        "PROJECT_ROOT = _find_project_root()\n",
        "os.chdir(PROJECT_ROOT)\n",
        "if PROJECT_ROOT not in sys.path:\n",
        "    sys.path.insert(0, PROJECT_ROOT)\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Contents: {os.listdir(PROJECT_ROOT)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Import Libraries and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'data'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-173637131.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_cifar100_dataloaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAverageMeter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvim_tiny_cifar100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Dict\n",
        "\n",
        "# Ensure project root is on path (run \"Setup Project Structure\" cell first; this fixes it if run out of order)\n",
        "def _find_root():\n",
        "    if os.path.exists(\"data\") and (os.path.exists(\"vim\") or os.path.exists(\"Vim\")):\n",
        "        return os.getcwd()\n",
        "    for path in [\"/content/Vim\", \"/content\", os.getcwd()]:\n",
        "        if os.path.exists(os.path.join(path, \"data\")) and (\n",
        "            os.path.exists(os.path.join(path, \"vim\")) or os.path.exists(os.path.join(path, \"Vim\"))\n",
        "        ):\n",
        "            return path\n",
        "    return os.getcwd()\n",
        "_root = _find_root()\n",
        "os.chdir(_root)\n",
        "if _root not in sys.path:\n",
        "    sys.path.insert(0, _root)\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.cuda import amp\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from data import build_cifar100_dataloaders\n",
        "from utils import AverageMeter, accuracy\n",
        "try:\n",
        "    from Vim import vim_tiny_cifar100\n",
        "except ImportError:\n",
        "    from Vim.vim import vim_tiny_cifar100\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration (tune for Colab: smaller batch_size if OOM)\n",
        "config = {\n",
        "    \"data_dir\": \"./data\",  # CIFAR-100 will download here\n",
        "    \"epochs\": 200,         # Reduce (e.g. 10) for a quick test run\n",
        "    \"batch_size\": 128,     # Reduce to 64 if GPU OOM\n",
        "    \"lr\": 1e-3,\n",
        "    \"weight_decay\": 0.05,\n",
        "    \"warmup_epochs\": 5,\n",
        "    \"img_size\": 32,\n",
        "    \"num_workers\": 2,      # Colab-friendly\n",
        "    \"seed\": 42,\n",
        "    \"grad_clip\": 1.0,\n",
        "    \"log_dir\": \"./runs/vim_cifar100\",\n",
        "    \"output_dir\": \"./checkpoints\",\n",
        "}\n",
        "\n",
        "# Optional: use Google Drive for data and checkpoints (run Drive mount cell first)\n",
        "# config[\"data_dir\"] = \"/content/drive/MyDrive/vim_data\"\n",
        "# config[\"output_dir\"] = \"/content/drive/MyDrive/vim_checkpoints\"\n",
        "\n",
        "# Create output directories\n",
        "Path(config[\"output_dir\"]).mkdir(parents=True, exist_ok=True)\n",
        "Path(config[\"log_dir\"]).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for k, v in config.items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Set Random Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "set_seed(config[\"seed\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading CIFAR-100 dataset...\")\n",
        "train_loader, val_loader = build_cifar100_dataloaders(\n",
        "    data_dir=config[\"data_dir\"],\n",
        "    img_size=config[\"img_size\"],\n",
        "    batch_size=config[\"batch_size\"],\n",
        "    num_workers=config[\"num_workers\"],\n",
        ")\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")\n",
        "print(f\"Batch size: {config['batch_size']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = vim_tiny_cifar100(img_size=config[\"img_size\"])\n",
        "model = model.to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params / 1e6:.2f}M\")\n",
        "print(f\"Trainable parameters: {trainable_params / 1e6:.2f}M\")\n",
        "\n",
        "# Test forward pass\n",
        "with torch.no_grad():\n",
        "    dummy_input = torch.randn(1, 3, config[\"img_size\"], config[\"img_size\"]).to(device)\n",
        "    dummy_output = model(dummy_input)\n",
        "    print(f\"Input shape: {dummy_input.shape}\")\n",
        "    print(f\"Output shape: {dummy_output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Setup Training Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "# Scheduler\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=config[\"epochs\"] - config[\"warmup_epochs\"])\n",
        "\n",
        "# Mixed precision scaler\n",
        "scaler = amp.GradScaler()\n",
        "\n",
        "# TensorBoard writer\n",
        "writer = SummaryWriter(log_dir=config[\"log_dir\"])\n",
        "\n",
        "print(\"Training components initialized.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(\n",
        "    model, criterion, optimizer, scaler, dataloader, device, epoch\n",
        ") -> Dict[str, float]:\n",
        "    model.train()\n",
        "    loss_meter = AverageMeter(\"loss\")\n",
        "    acc1_meter = AverageMeter(\"acc1\")\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch} [train]\")\n",
        "    for images, targets in pbar:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with amp.autocast():\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config[\"grad_clip\"])\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        acc1, = accuracy(outputs.detach(), targets, topk=(1,))\n",
        "        loss_meter.update(loss.item(), images.size(0))\n",
        "        acc1_meter.update(acc1.item(), images.size(0))\n",
        "\n",
        "        pbar.set_postfix(loss=loss_meter.avg, acc1=acc1_meter.avg)\n",
        "\n",
        "    writer.add_scalar(\"train/loss\", loss_meter.avg, epoch)\n",
        "    writer.add_scalar(\"train/acc1\", acc1_meter.avg, epoch)\n",
        "\n",
        "    return {\"loss\": loss_meter.avg, \"acc1\": acc1_meter.avg}\n",
        "\n",
        "\n",
        "def validate(model, criterion, dataloader, device, epoch) -> Dict[str, float]:\n",
        "    model.eval()\n",
        "    loss_meter = AverageMeter(\"loss\")\n",
        "    acc1_meter = AverageMeter(\"acc1\")\n",
        "    acc5_meter = AverageMeter(\"acc5\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch} [val]\")\n",
        "        for images, targets in pbar:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            acc1, acc5 = accuracy(outputs, targets, topk=(1, 5))\n",
        "\n",
        "            loss_meter.update(loss.item(), images.size(0))\n",
        "            acc1_meter.update(acc1.item(), images.size(0))\n",
        "            acc5_meter.update(acc5.item(), images.size(0))\n",
        "\n",
        "            pbar.set_postfix(loss=loss_meter.avg, acc1=acc1_meter.avg, acc5=acc5_meter.avg)\n",
        "\n",
        "    writer.add_scalar(\"val/loss\", loss_meter.avg, epoch)\n",
        "    writer.add_scalar(\"val/acc1\", acc1_meter.avg, epoch)\n",
        "    writer.add_scalar(\"val/acc5\", acc5_meter.avg, epoch)\n",
        "\n",
        "    return {\"loss\": loss_meter.avg, \"acc1\": acc1_meter.avg, \"acc5\": acc5_meter.avg}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_acc1 = 0.0\n",
        "epochs_no_improve = 0\n",
        "patience = 20\n",
        "\n",
        "train_history = {\"loss\": [], \"acc1\": []}\n",
        "val_history = {\"loss\": [], \"acc1\": [], \"acc5\": []}\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(f\"Total epochs: {config['epochs']}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for epoch in range(1, config[\"epochs\"] + 1):\n",
        "    # Learning rate warmup\n",
        "    if epoch <= config[\"warmup_epochs\"]:\n",
        "        warmup_factor = epoch / float(max(1, config[\"warmup_epochs\"]))\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group[\"lr\"] = config[\"lr\"] * warmup_factor\n",
        "    else:\n",
        "        scheduler.step()\n",
        "\n",
        "    # Train\n",
        "    train_stats = train_one_epoch(\n",
        "        model, criterion, optimizer, scaler, train_loader, device, epoch\n",
        "    )\n",
        "    \n",
        "    # Validate\n",
        "    val_stats = validate(model, criterion, val_loader, device, epoch)\n",
        "\n",
        "    # Save history\n",
        "    train_history[\"loss\"].append(train_stats[\"loss\"])\n",
        "    train_history[\"acc1\"].append(train_stats[\"acc1\"])\n",
        "    val_history[\"loss\"].append(val_stats[\"loss\"])\n",
        "    val_history[\"acc1\"].append(val_stats[\"acc1\"])\n",
        "    val_history[\"acc5\"].append(val_stats[\"acc5\"])\n",
        "\n",
        "    # Checkpointing\n",
        "    is_best = val_stats[\"acc1\"] > best_acc1\n",
        "    if is_best:\n",
        "        best_acc1 = val_stats[\"acc1\"]\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    ckpt = {\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"optimizer_state\": optimizer.state_dict(),\n",
        "        \"scaler_state\": scaler.state_dict(),\n",
        "        \"best_acc1\": best_acc1,\n",
        "        \"config\": config,\n",
        "    }\n",
        "    \n",
        "    # Save checkpoint\n",
        "    ckpt_path = Path(config[\"output_dir\"]) / f\"checkpoint_{epoch:03d}.pth\"\n",
        "    torch.save(ckpt, ckpt_path)\n",
        "    \n",
        "    if is_best:\n",
        "        best_path = Path(config[\"output_dir\"]) / \"checkpoint_best.pth\"\n",
        "        torch.save(ckpt, best_path)\n",
        "        print(f\"\\n✓ New best model! Val Acc1: {best_acc1:.2f}%\")\n",
        "\n",
        "    # Print epoch summary\n",
        "    print(f\"\\nEpoch {epoch}/{config['epochs']}:\")\n",
        "    print(f\"  Train Loss: {train_stats['loss']:.4f}, Train Acc1: {train_stats['acc1']:.2f}%\")\n",
        "    print(f\"  Val Loss: {val_stats['loss']:.4f}, Val Acc1: {val_stats['acc1']:.2f}%, Val Acc5: {val_stats['acc5']:.2f}%\")\n",
        "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Early stopping\n",
        "    if epochs_no_improve >= patience:\n",
        "        print(f\"\\nEarly stopping triggered after {epoch} epochs (no improvement for {patience} epochs).\")\n",
        "        break\n",
        "\n",
        "writer.close()\n",
        "print(\"\\nTraining completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Plot Training Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_history[\"loss\"], label=\"Train\")\n",
        "plt.plot(val_history[\"loss\"], label=\"Val\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_history[\"acc1\"], label=\"Train@1\")\n",
        "plt.plot(val_history[\"acc1\"], label=\"Val@1\")\n",
        "plt.plot(val_history[\"acc5\"], label=\"Val@5\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.legend()\n",
        "plt.title(\"Training and Validation Accuracy\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"training_curves.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Best validation accuracy: {best_acc1:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. TensorBoard in Colab\n",
        "\n",
        "Run the cell below to launch TensorBoard. Use the link shown to view training curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./runs --port 6006"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Save to Google Drive (Optional)\n",
        "\n",
        "Run the cell below to mount Drive and copy checkpoints (or set config paths earlier)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# import shutil\n",
        "# \n",
        "# drive.mount('/content/drive')\n",
        "# \n",
        "# # Copy checkpoints to Drive\n",
        "# drive_path = '/content/drive/MyDrive/vim_checkpoints'\n",
        "# shutil.copytree(config['output_dir'], drive_path, dirs_exist_ok=True)\n",
        "# print(f\"Checkpoints saved to {drive_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Load Best Model and Evaluate (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best checkpoint and run final evaluation (no TensorBoard logging)\n",
        "best_ckpt_path = Path(config[\"output_dir\"]) / \"checkpoint_best.pth\"\n",
        "if best_ckpt_path.exists():\n",
        "    ckpt = torch.load(best_ckpt_path, map_location=device)\n",
        "    model.load_state_dict(ckpt[\"model_state\"])\n",
        "    print(f\"Loaded best model from epoch {ckpt['epoch']} with Acc1: {ckpt['best_acc1']:.2f}%\")\n",
        "\n",
        "    model.eval()\n",
        "    loss_meter = AverageMeter(\"loss\")\n",
        "    acc1_meter = AverageMeter(\"acc1\")\n",
        "    acc5_meter = AverageMeter(\"acc5\")\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm(val_loader, desc=\"Evaluating\"):\n",
        "            images, targets = images.to(device), targets.to(device)\n",
        "            outputs = model(images)\n",
        "            loss_meter.update(criterion(outputs, targets).item(), images.size(0))\n",
        "            a1, a5 = accuracy(outputs, targets, topk=(1, 5))\n",
        "            acc1_meter.update(a1.item(), images.size(0))\n",
        "            acc5_meter.update(a5.item(), images.size(0))\n",
        "    print(f\"\\nFinal validation: Loss {loss_meter.avg:.4f}, Top-1 {acc1_meter.avg:.2f}%, Top-5 {acc5_meter.avg:.2f}%\")\n",
        "else:\n",
        "    print(\"No checkpoint_best.pth found. Train first or check output_dir.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Quick test**: Set `config[\"epochs\"] = 10` in the config cell to run a short sanity check before full training."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
